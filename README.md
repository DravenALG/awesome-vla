<div align="center">

# ğŸ¤– Awesome VLA

**ğŸ“œ A Curated List of Vision-Language-Action (VLA) Research** </br>
*Based on [Awesome World Models](https://github.com/knightnemo/Awesome-World-Models/tree/main).*

<p align="center">
  <img src="awesome-vla.jpg" alt="Awesome VLA" width="100%" style="border-radius: 15px; box-shadow: 0 4px 24px rgba(0,0,0,.1); margin: 5px 0;">
</p>

*Photo Credit: [Gemini-Nano-BananağŸŒ](https://aistudio.google.com/models/gemini-3-pro-image)*.

</div>

## Overview
- ğŸ¯ [Aim](#aim)
- ğŸ“š [Definition](#definition)
- ğŸ“– [Survey](#survey)
- ğŸ§  [General VLA](#general-vla)
- ğŸŒ [VLA with 3D Spatial Modelling](#vla-with-3d-spatial-modelling)
- ğŸ”¥ [VLA with Post-Training (e.g., RL)](#vla-with-post-training-eg-rl)
- ğŸ§© [VLA with Intermediate Modelling (e.g., World Modelling, Reasoning)](#vla-with-intermediate-modelling-eg-world-modelling-reasoning)
- ğŸ§ª [VLA with Latent Actions](#vla-with-latent-actions)
- ğŸª¶ [Efficient VLA](#efficient-vla)
- ğŸ§­ [Domain-Specific VLA (Humanoid, Tactile)](#domain-specific-vla-humanoid-tactile)
- ğŸ§· [Other Topics in VLA](#other-topics-in-vla)
- ğŸ¦¾ [Traditional Policies](#traditional-policies)
- ğŸ“Š [Benchmarks](#benchmarks)

## Aim
This is a curated list of VLA research that systematically organizes various topics within the field. It will be continuously updated and refined, with the goal of clarifying the research context for scholars in the VLA domain. If you have any new papers worth adding, please feel free to push and join us in maintaining a high-quality VLA list.

## Definition
In short, VLA models are a type of robotics policy that inherts the pretrained VLMsâ€™ rich language grounding and visual understanding abilities to offter a scalable route toward general-purpose, language-conditioned robot policies. We can trace the origin and formal definition of the VLA to the work TR-2.

- [â­ï¸] **RT-2**, RT-2: Vision-Language-Action Models Transfer Web Knowledge to Robotic Control. [![arXiv](https://img.shields.io/badge/arXiv-2307.15818-b31b1b.svg)](https://arxiv.org/abs/2307.15818) [![Website](https://img.shields.io/badge/Website-Link-blue)](https://robotics-transformer2.github.io)

## Survey
- A Survey on Vision-Language-Action Models for Embodied AI. [![arXiv](https://img.shields.io/badge/arXiv-2405.14093-b31b1b.svg)](https://arxiv.org/abs/2405.14093) [![Website](https://img.shields.io/badge/Website-Link-blue)](https://github.com/yueen-ma/Awesome-VLA)

- Vision-Language-Action (VLA) Models: Concepts, Progress, Applications and Challenges. [![arXiv](https://img.shields.io/badge/arXiv-2505.04769-b31b1b.svg)](https://arxiv.org/abs/2505.04769) [![Website](https://img.shields.io/badge/Website-Link-blue)](https://github.com/Applied-AI-Research-Lab/Vision-Language-Action-Models-Concepts-Progress-Applications-and-Challenges)

## General VLA
<!-- TODO: fill in -->


## VLA with 3D Spatial Modelling
<!-- TODO: fill in -->


## VLA with Post-Training (e.g., RL)
<!-- TODO: fill in -->


## VLA with Intermediate Modelling (e.g., World Modelling, Reasoning)
<!-- TODO: fill in -->


## VLA with Latent Actions
<!-- TODO: fill in -->


## Efficient VLA
<!-- TODO: fill in -->


## Domain-Specific VLA (Humanoid, Tactile)
<!-- TODO: fill in -->


## Other Topics in VLA
<!-- TODO: fill in -->


## Traditional Policies
<!-- TODO: fill in -->


## Benchmarks
<!-- TODO: fill in -->
